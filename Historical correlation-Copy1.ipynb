{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical correlations\n",
    "\n",
    "This tutorial on correlation was inspired by us joking about the relationship between changes in Bitcoin prices and the S&P 500 American stock index.\n",
    "\n",
    "Preliminary instructions:\n",
    "\n",
    "1. Install Python. [Pyenv](https://github.com/pyenv/pyenv/) is absolutely essential.\n",
    "1. It's best practice to create a Python [virtual environment](https://docs.python.org/3/library/venv.html) but the simplest way to install dependencies is to run `pip install numpy pandas plotly`.\n",
    "1. Download all historic data from Yahoo Finance for [SPY](https://finance.yahoo.com/quote/XLF/history?p=SPY) and [XLF](https://finance.yahoo.com/quote/XLF/history?p=XLF), the S&P 500 ETF and the S&P Finance ETF (which tracks a subset of the whole S&P 500 index). Yahoo will let you download a CSV file containing data since the 1990s.\n",
    "\n",
    "**Contact** [Ahmed Fasih](https://fasiha.github.io/#contact). Open an issue on [GitHub](https://github.com/fasiha/historic-correlations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use Pandas, which is a big but very well-designed and well-documented library, to load the two spreadsheets, merge them, and save their one-day percent change in a dataframe called `df` under `df.spy1` and `df.xlf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "spy = pd.read_csv('SPY.csv', parse_dates=['Date'])\n",
    "xlf = pd.read_csv('XLF.csv', parse_dates=['Date'])\n",
    "\n",
    "xlf['xlf'] = xlf['Close']\n",
    "spy['spy'] = spy['Close']\n",
    "\n",
    "xlf = xlf.loc[:, ['Date', 'xlf']]\n",
    "spy = spy.loc[:, ['Date', 'spy']]\n",
    "\n",
    "xlf = xlf.set_index('Date')\n",
    "spy = spy.set_index('Date')\n",
    "\n",
    "df = pd.concat([spy, xlf], axis=1).dropna()\n",
    "df['spy1'] = df.spy.pct_change()*100\n",
    "df['xlf1'] = df.xlf.pct_change()*100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the two time series (i.e., vectors of data). I like Plotly because it outputs interactive graphs for the browser, unlike dead pixels output by Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df.spy, name='SPY', line=dict(width=4)))\n",
    "fig.add_trace(go.Scatter(x=df.index, y=df.xlf, name='XLF'), secondary_y=True)\n",
    "fig.update_layout(title_text='SPY (left) and XLF (right), {} to {} (data: Yahoo Finance)'.format(str(df.index[0])[:10], str(df.index[-1])[:10]))\n",
    "fig.update_xaxes(title_text=\"Date\")\n",
    "fig.update_yaxes(title_text=\"SPY\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"XLF\", secondary_y=True)\n",
    "fig.update_layout(hovermode=\"x\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a scatter plot of the one-day percent returns of SPY and XLF (for the dates where we have prices for both)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=df.spy1,\n",
    "    y=df.xlf1,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=list(map(lambda x: x.year + x.month/12, df.index.to_pydatetime())),\n",
    "        colorscale='Viridis',\n",
    "        showscale=True\n",
    "    ),\n",
    "    hovertemplate = 'SPY: %{x:.2f}%, XLF: %{y:.2f}%<br>(%{text})',\n",
    "    text=df.index,\n",
    ")])\n",
    "fig.update_layout(\n",
    "    title='{} to {} (data: Yahoo Finance)'.format(str(df.index[0])[:10], str(df.index[-1])[:10]),\n",
    "    xaxis_title=\"SPY 1 day % change\",\n",
    "    yaxis_title=\"XLF 1 day % change\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation quantifies the perfection of the linear relationship between two series of numbers. A scatter plot of two perfectly correlated variables would show a straight line (the slope doesn't matter).\n",
    "\n",
    "These two sets of numbers, SPY's percent change on the x axis and XLF's on the y axis, are highly correlated, though not perfectly so. [Wikipedia](https://en.wikipedia.org/w/index.php?title=Pearson_correlation_coefficient&oldid=951002692#For_a_population) gives a handy equation for the correlation coefficient, specifically the Pearson coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def mycorrcoef(x, y):\n",
    "    m = np.mean\n",
    "    value = m(x * y) - m(x) * m(y)\n",
    "    normalize = np.sqrt( (m(x*x) - m(x)**2) * (m(y*y) - m(y)**2) )\n",
    "    return value / normalize\n",
    "\n",
    "print(mycorrcoef(df.dropna().spy1, df.dropna().xlf1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect correlation would yield 1.0 or -1.0 (depending on whether the slope was positive or negative). We're pretty close to that: 0.84. Recall that XLF is a subset of SPY, specifically the financial firms, along with ten other industries.\n",
    "\n",
    "Here are a couple of other ways to compute that same correlation, using Numpy and Pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Numpy\n",
    "print(np.corrcoef(df.dropna().spy1, df.dropna().xlf1)[0, 1])\n",
    "\n",
    "# using pandas\n",
    "print(df.corr().loc['spy1','xlf1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thing is, this one number collapses twenty-two years of history into a single number. I wonder: what's the correlation between daily returns of SPY vs XLF in the first week of this dataset? The first year? The first five years?\n",
    "\n",
    "Pandas can answer this question, with one of its magically helpful functions, [`expanding`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.expanding.html): it constructs an expanding window over the dataframe and evaluates the correlation over each point along the expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=df.index, y=df.spy1.expanding(5).corr(df.xlf1), mode='lines'),\n",
    "                layout=dict(\n",
    "                    title='SPY vs XLF daily % change, correlation in data window starting from {}'.format(str(df.index[0])[:10]),\n",
    "                    xaxis_title=\"Data window end date\",\n",
    "                    yaxis_title=\"correlation\",\n",
    "            ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm your intuition: the rightmost value in this chart represents the correlation between the *entirety* of the dataset and corresponds to that 0.84 we computed above.\n",
    "\n",
    "Similarly, the minimum of this graph, 0.68 in December 2000, corresponds to the correlation between these two datasets between 1998 (the start) and December 2000.\n",
    "\n",
    "In other words, this graph shows what correlations your `mycorrcoef` function would have printed over history.\n",
    "\n",
    "But we can see that the correlation between these two datasets has changed over time. It was roughly 80% (0.8 on the y axis) before the 2000 crash that accompanied the popping of the Tech Bubble, and for several years thereafter, XLF daily returns were less correlated than SPY returns.\n",
    "\n",
    "I'm also curiousâ€”what are the correlations over the last week, last month, last year of the data? That is, for the time windows of varying lengths ending *today*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=df.index[::-1], y=df.spy1[::-1].expanding(5).corr(df.xlf1[::-1]), mode='lines'),\n",
    "               layout=dict(\n",
    "                    title='SPY vs XLF daily % change, correlation in data window ending {}'.format(str(df.index[-1])[:10]),\n",
    "                    xaxis_title=\"Data window <i>start</i> date\",\n",
    "                    yaxis_title=\"correlation\",\n",
    "               ))\n",
    "fig.update_layout(\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, you can confirm that the *leftmost* point in the graph here corresponds to the global correlation we calculated above, 0.84.\n",
    "\n",
    "Both the above graphs show the correlation over *expanding* windows of time: from 1998 onwards or from 2020 backwards. Suppose I wanted to make a service that printed the *trailing* correlation over a *fixed* window, that is, what the correlation between SPY and XLF daily returns has been over the last month, the last six months, the last year?\n",
    "\n",
    "Pandas has more magic, called [`rolling`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rolling.html), to answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=df.index,\n",
    "    y=df.spy1.rolling(win).corr(df.xlf1),\n",
    "    mode='lines',\n",
    "    name='{} sessions'.format(win),\n",
    "    line=dict(width=width),\n",
    "    opacity=0.75\n",
    ") for win, width in zip([240, 120, 20], [4, 2, 1])], \n",
    "                layout=dict(\n",
    "                    title='Trailing correlations for several time horizons: SPY vs XLF daily percent returns',\n",
    "                    xaxis_title='Window end date',\n",
    "                    yaxis_title='correlation',\n",
    "                    hovermode='x'\n",
    "                ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm your understanding: if you published a newsletter that included the most recent month's correlations between SPY and XLF's daily returns, the thin \"20 sessions\" curve above gives the number you would have published on any given date.\n",
    "\n",
    "This is most interesting. Even though the correlation of these two time series over all time was 0.84, we see that over shorter windows, the correlations move around a lot.\n",
    "\n",
    "It almost seems a periodic cycleâ€”the trailing monthly correlation hits near 1.0, meaning SPY and XLF's daily returns nearly moved in lock-step (though the correlation removes the absolute slope between the two). And with apparently just as much regularily, the one-month correlation drops substantially.\n",
    "\n",
    "We see that the Tech Bubble saw significant decorrelation between financials and the broader S&P 500, possibly because SPY was heavily tech-weighted, and those stocks saw the most massive declines. We note that in contrast we can't really detect the 2008 Great Financial Crisis in these three curvesâ€”the declines in the broader index seem to have been quite correlated with those in financials since by then SPY was heavily-weighted towards financials.\n",
    "\n",
    "So we can go the full distance: we can make a chart that shows the correlation between these two time series between any start and end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = np.round(np.linspace(5, len(df) - 1, 500)).astype(int)\n",
    "p = pd.DataFrame([df.spy1.rolling(win).corr(df.xlf1) for win in wins],\n",
    "                 index=wins)\n",
    "# Going down `p`'s rows changes the length of the window of the correlation.\n",
    "# Going across `p`'s columns changes the end date of the window of the correlation.\n",
    "# Now change this from *window size* to *start date*.\n",
    "for i in range(p.values.shape[1]):\n",
    "    n = np.sum(np.isfinite(p.values[:, i]))\n",
    "    p.values[:n, i] = np.flip(p.values[:n, i])\n",
    "# update the index (the value as you run down columns) from window size to start date\n",
    "p.index = df.index[wins]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we make a new Pandas dataframe to contain the correlations between any start and end date. Let's plot it below with Plotly as a heatmap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "                    z=p,\n",
    "                    x=p.columns,\n",
    "                    y=p.index,\n",
    "                    colorscale='Viridis',\n",
    "                    zmin=0.5,\n",
    "                    zmax=.95,\n",
    "                    hovertemplate = '%{y}â€”%{x} corr: %{z:.2f}',\n",
    "                ),\n",
    "                layout=dict(\n",
    "                    title='Correlation over all time windows: SPY vs XLF daily percent returns',\n",
    "                    xaxis_title='Window end date',\n",
    "                    yaxis_title='Window start date',\n",
    "                ))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can verify by hovering above the heatmap, the 0.84 overall correlation corresponds to the lower-right pixel, i.e., the correlation between 1998 and recently. If you published a heatmap like this in a newsletter, the first one would be the part of the triangle with the x axis (window end date) equal to the date then. Over the years you'd get successively larger and larger triangles as you moved to the right of the x axis.\n",
    "\n",
    "This lets us see in total detail what that 0.84 overall correlation between SPY and XLF daily returns really means. There are several epochs that we can break down the last twenty-two years into.\n",
    "1. The dark triangle in the lower-left corresponds to the period of low-ish correlation between SPY and XLF daily returns during the Tech Bubble popping.\n",
    "1. From roughly mid-2000 to mid-2008, SPY and XLF daily returns were very highly correlated. All time horizons that started and ended within this band saw correlation of roughly 0.9. This is the band of yellow in the lower-left (and to the right of the darker wedge). The transition between the Tech Bubble to this period is quite obvious in this graph.\n",
    "1. However, another major transition is quite visible starting around September 2008. Recall that September to October 2008 saw a trapdoor open under SPY, when it lost roughly 30% in a month (after it was around 20% off the 2007 October peak; sorry my Plotly skills aren't advanced enough to give you an interactive percent-change version of the first plot above to verify this right now). This means that every time horizon that began before the September 2008 inflection point and ended after it falls in that big rectangle of smooth green, of correlation roughly 0.85, which time horizons fully on either side of this inflection point have more opportunity to vary from 0.85. To understand this, scroll back up to the scatter plot of daily percent changes and note how all the dots that ring the central line, like a halo, are from this mid-2008 period. Those deviations from linearity form an envelope inside which all other points lie, and so the correlation over any time horizon that includes that halo is forced to be close to 0.85.\n",
    "1. Other bands, both vertical and horizontal, are visible in the data after the Great Financial Crisis.\n",
    "\n",
    "Hopefully this tutorial offers the basics of using correlation to break down two time series. The statistics and signal processing of such data is very rich, so there are many, many more directions you can take such analysis.\n",
    "\n",
    "Potentially interesting reading:\n",
    "- [Eduardo Sasso](https://eduardosasso.co/blog/turning-my-obsession-in-the-stock-market-into-a-side-project/)'s newsletter, Bullish."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('3.8.1': pyenv)",
   "language": "python",
   "name": "python38164bit381pyenv59cbade6d69a492d8a5041a1a8a966d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
